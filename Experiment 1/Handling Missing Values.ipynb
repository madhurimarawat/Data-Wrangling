{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "370aaf67",
   "metadata": {},
   "source": [
    "## Experiment 1\n",
    "\n",
    "### Handling Missing Values: Identify and fill missing values in a dataset using methods such as mean imputation or forward/backward filling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c5083e",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "### **1. Data Cleaning**\n",
    "\n",
    "Data cleaning is the essential process of detecting and correcting errors, inconsistencies, or inaccuracies in a dataset. It ensures the data is reliable, useful, and suitable for analysis. This step involves identifying irrelevant data, correcting inaccuracies, and ensuring consistency in data formatting. Clean data improves the quality of insights and enhances the performance of predictive models. It can include:\n",
    "\n",
    "- Identifying erroneous values or outliers.\n",
    "- Correcting or removing faulty or inconsistent entries.\n",
    "- Handling data quality issues like redundant or duplicated data.\n",
    "\n",
    "### **2. Handling Missing Values**\n",
    "\n",
    "Handling missing values is critical to maintaining the integrity of the dataset. This step involves identifying and addressing missing data points using various techniques. Depending on the situation and data type, missing values can be handled in two ways:\n",
    "\n",
    "- **Removal**: If the missing values are sparse and spread out across the dataset, it might be feasible to remove the rows or columns with missing data, but only if their absence doesn't significantly reduce the quality or representativeness of the dataset.\n",
    "- **Imputation**: For larger gaps in data, missing values are filled in using statistical methods:\n",
    "    - **Mean/Median Imputation**: Replaces missing values with the mean or median of the existing data in that column.\n",
    "    - **Mode Imputation**: Useful for categorical data, where the most frequent value is used to fill missing data.\n",
    "    - **Forward/Backward Filling**: In time series data, missing values are filled using the preceding (forward fill) or subsequent (backward fill) data points.\n",
    "\n",
    "### **3. Removing Duplicate Data**\n",
    "\n",
    "Duplicate data can skew analysis and lead to inaccurate insights, so removing duplicates is a key data cleaning step. This involves identifying rows or entries that have been repeated unintentionally. Duplicate entries can arise from data collection errors or merging datasets. The process involves:\n",
    "\n",
    "- Detecting duplicated rows or entries based on certain fields.\n",
    "- Deciding which duplicates to keep and which to remove (e.g., keeping the first or last instance).\n",
    "\n",
    "Eliminating duplicates helps streamline the dataset, reduce redundancy, and improve the performance of machine learning models.\n",
    "\n",
    "### **4. Standardizing Data Formats**\n",
    "\n",
    "Standardization ensures that the data in your dataset follows a consistent format. This includes uniformity in how dates, times, currencies, and categorical variables are represented. Common issues include inconsistent date formats (e.g., MM/DD/YYYY vs. DD/MM/YYYY), mixed use of text cases in strings, and varying units for measurements.\n",
    "\n",
    "- **Date & Time Standardization**: All date fields should follow a uniform format (e.g., ISO 8601: YYYY-MM-DD).\n",
    "- **Text Formatting**: Text data should be standardized in terms of case (upper/lower) and spelling (U.S. vs British English).\n",
    "- **Numeric Data Standardization**: Numeric fields such as currency and measurements should be standardized in the same unit (e.g., meters, dollars).\n",
    "\n",
    "Standardization ensures that the data is interpretable by both humans and machines and is crucial for further analysis.\n",
    "\n",
    "### **5. Removing Outliers**\n",
    "\n",
    "Outliers are data points that deviate significantly from the rest of the dataset. These points can be caused by data entry errors, measurement anomalies, or natural variations in the data. While outliers can sometimes hold valuable insights, they often distort the results of statistical analyses or machine learning models. The steps involved in handling outliers include:\n",
    "\n",
    "- **Detection**: Identifying outliers using statistical techniques such as the interquartile range (IQR) method, Z-scores, or visual methods like box plots.\n",
    "- **Removal or Transformation**: Depending on their impact, outliers can be removed, capped, or transformed to fit within an acceptable range.\n",
    "  \n",
    "By addressing outliers, you ensure that your model predictions are not unduly influenced by extreme values.\n",
    "\n",
    "### **6. Data Transformation**\n",
    "\n",
    "Data transformation involves modifying or converting data into a usable format that better suits analysis or modeling. This step is essential for scaling, normalizing, or encoding data. It can include:\n",
    "\n",
    "- **Normalization/Standardization**: Adjusting the scale of numerical values so they have comparable ranges (e.g., scaling all features between 0 and 1).\n",
    "- **Encoding Categorical Variables**: Converting categorical data into numerical form through one-hot encoding, label encoding, or other techniques.\n",
    "- **Log Transformation**: Used to handle skewed data by compressing the range of variables and reducing the effect of outliers.\n",
    "\n",
    "Data transformation improves the efficiency of algorithms, making patterns more discernible and reducing computational complexity.\n",
    "\n",
    "### **7. Data Integration**\n",
    "\n",
    "Data integration involves merging data from different sources to create a unified dataset. This is especially important when combining data from disparate systems like databases, spreadsheets, or APIs. Integration allows for a more comprehensive view of the data, enabling richer analysis. Key tasks in data integration include:\n",
    "\n",
    "- **Matching Schema**: Ensuring that the structure (schema) of different datasets aligns so that they can be combined without errors.\n",
    "- **Joining Data**: Using methods such as joins (inner, outer, left, or right joins) to combine datasets on key attributes.\n",
    "- **Handling Redundancy**: Removing duplicate or redundant data points created during the merging process.\n",
    "\n",
    "Data integration enables more holistic insights and supports better decision-making across multiple data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06460af4",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589d63ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.3\n"
     ]
    }
   ],
   "source": [
    "# For Data handling and other operations\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# For Ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# To see the version of pandas\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424b1f6d",
   "metadata": {},
   "source": [
    "### Mean Imputation\n",
    "\n",
    "#### Mean Imputation is a method where missing values in a dataset are replaced with the mean (average) of the available values for that particular variable or column. This approach maintains the dataset's overall statistical properties but can reduce variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ac1501f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "     A    B    C\n",
      "0  1.0  NaN  1.0\n",
      "1  2.0  2.0  NaN\n",
      "2  NaN  3.0  3.0\n",
      "3  4.0  NaN  4.0\n",
      "4  5.0  5.0  NaN\n",
      "\n",
      "DataFrame after Mean Imputation:\n",
      "     A         B         C\n",
      "0  1.0  3.333333  1.000000\n",
      "1  2.0  2.000000  2.666667\n",
      "2  3.0  3.000000  3.000000\n",
      "3  4.0  3.333333  4.000000\n",
      "4  5.0  5.000000  2.666667\n"
     ]
    }
   ],
   "source": [
    "# Mean Imputation\n",
    "# Imputing missing values with mean value\n",
    "\n",
    "# Sample dataset with missing values\n",
    "data = {\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [np.nan, 2, 3, np.nan, 5],\n",
    "    'C': [1, np.nan, 3, 4, np.nan]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Mean Imputation\n",
    "df_mean_imputed = df.fillna(df.mean())\n",
    "\n",
    "# Display the DataFrame after Mean Imputation\n",
    "print(\"\\nDataFrame after Mean Imputation:\")\n",
    "print(df_mean_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4a8675",
   "metadata": {},
   "source": [
    "### Forward Filling\n",
    "\n",
    "### **Forward Filling** is a method used to handle missing data by filling the missing values with the last observed non-missing value in the column. This technique is commonly applied in time series data to propagate the most recent valid value forward until a new valid observation is encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69080154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "     A    B    C\n",
      "0  1.0  NaN  1.0\n",
      "1  2.0  2.0  NaN\n",
      "2  NaN  3.0  3.0\n",
      "3  4.0  NaN  4.0\n",
      "4  5.0  5.0  NaN\n",
      "\n",
      "DataFrame after Forward Filling:\n",
      "     A    B    C\n",
      "0  1.0  NaN  1.0\n",
      "1  2.0  2.0  1.0\n",
      "2  2.0  3.0  3.0\n",
      "3  4.0  3.0  4.0\n",
      "4  5.0  5.0  4.0\n"
     ]
    }
   ],
   "source": [
    "# Forward Filling\n",
    "# This program fills missing values with the previous value in the column.\n",
    "\n",
    "# Display the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Forward Filling\n",
    "df_forward_filled = df.fillna(method='ffill')\n",
    "\n",
    "# Display the DataFrame after Forward Filling\n",
    "print(\"\\nDataFrame after Forward Filling:\")\n",
    "print(df_forward_filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487e07fe",
   "metadata": {},
   "source": [
    "### Backward Filling\n",
    "#### Backward Filling is a technique for handling missing data by filling the missing values with the next valid observation in the column. In time-series data, this means that the missing value is replaced by the next available value as you move backward in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a528d425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "     A    B    C\n",
      "0  1.0  NaN  1.0\n",
      "1  2.0  2.0  NaN\n",
      "2  NaN  3.0  3.0\n",
      "3  4.0  NaN  4.0\n",
      "4  5.0  5.0  NaN\n",
      "\n",
      "DataFrame after Backward Filling:\n",
      "     A    B    C\n",
      "0  1.0  2.0  1.0\n",
      "1  2.0  2.0  3.0\n",
      "2  4.0  3.0  3.0\n",
      "3  4.0  5.0  4.0\n",
      "4  5.0  5.0  NaN\n"
     ]
    }
   ],
   "source": [
    "# Backward Filling\n",
    "# This program fills missing values with the next value in the column.\n",
    "\n",
    "# Display the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Backward Filling\n",
    "df_backward_filled = df.fillna(method='bfill')\n",
    "\n",
    "# Display the DataFrame after Backward Filling\n",
    "print(\"\\nDataFrame after Backward Filling:\")\n",
    "print(df_backward_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c0cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
